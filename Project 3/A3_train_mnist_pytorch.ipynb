{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TOyGrPT5ASDc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim, nn, unsqueeze\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "colab_type": "code",
        "id": "sZD2NGz2Ak6w",
        "outputId": "74eec0da-d867-406b-be2c-4013a2162bf1"
      },
      "outputs": [],
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset0 = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=True, transform=transform)\n",
        "testset = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=False, transform=transform)\n",
        "\n",
        "# Also create a validation set \n",
        "trainset, valset = train_test_split(trainset0, test_size=10000, random_state=42)\n",
        "#pytorch alternative for spliting into train and validation sets\n",
        "#https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([16, 1, 28, 28])\n",
            "torch.Size([16])\n"
          ]
        }
      ],
      "source": [
        "training_data = enumerate(trainloader)\n",
        "batch_idx, (images, labels) = next(training_data)\n",
        "print(type(images)) # Checking the datatype \n",
        "print(images.shape) # the size of the image\n",
        "print(labels.shape) # the size of the labels\n",
        "\n",
        "#64: Represents 64 images\n",
        "# 1 : One color channel ==>> Grayscale\n",
        "# 28 by 28 pixel: the shape of these images so we can visualize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "colab_type": "code",
        "id": "XCsoAdjdLjPb",
        "outputId": "6e7a5f80-f945-4e5c-c538-8ef445b6ad3e"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.conv_neural_network_layers = nn.Sequential(\n",
        "                # output_sizeOfEachConvLayer = [(in_channel + 2*padding - kernel_size) / stride] + 1\n",
        "\n",
        "                # We have in_channels=1 because our input is a grayscale image\n",
        "                nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1, stride=1), # (N, 1, 28, 28) \n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size=2), \n",
        "\n",
        "                # output of second conv layer\n",
        "                nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size=2), \n",
        "\n",
        "                # output of third conv layer\n",
        "                nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size=2) )\n",
        "\n",
        "        # Linear layer\n",
        "        self.linear_layers = nn.Sequential(\n",
        "                nn.Linear(64*3*3, 10))\n",
        "                # nn.Linear(16, 10)) # The output is 10 which should match the size of our class\n",
        "\n",
        "\n",
        "    # Defining the forward pass \n",
        "    def forward(self, x):\n",
        "        x = self.conv_neural_network_layers(x)\n",
        "        # After we get the output of our convolutional layer we must flatten it or rearrange the output into a vector\n",
        "        x = torch.flatten(x, 1) # same as x = x.view(x.size(0), -1)\n",
        "        # Then pass it through the linear layer\n",
        "        x = self.linear_layers(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = Network()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss() #nn.BCELoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "#In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters.\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        y_hot = F.one_hot(y, 10)\n",
        "        y_hot = torch.zeros(batch_size, 10)\n",
        "        y_hot[range(y_hot.shape[0]), y]=1      \n",
        "\n",
        "        X, y_hot = X.to(device), y_hot.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y_hot)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "#We also check the model’s performance against the test dataset to ensure it is learning.\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        y_hot = F.one_hot(y, 10)\n",
        "        y_hot = torch.zeros(batch_size, 10)\n",
        "        y_hot[range(y_hot.shape[0]), y]=1      \n",
        "\n",
        "        X, y_hot = X.to(device), y_hot.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        test_loss += loss_fn(pred, y_hot).item()\n",
        "        correct += (pred.argmax(axis=1) == y_hot.argmax(axis=1)).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= 10000\n",
        "    correct /= size\n",
        "\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdb3a972310>\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/brunaguedes/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/Users/brunaguedes/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1295, in _shutdown_workers\n",
            "    if self._persistent_workers or self._workers_status[worker_id]:\n",
            "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 98.4%, Avg loss: 0.000798 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.000526 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.000572 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.000552 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.000513 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.000533 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.3%, Avg loss: 0.000476 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.000584 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.2%, Avg loss: 0.000547 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.2%, Avg loss: 0.000596 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.2%, Avg loss: 0.000586 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.000774 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.000723 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.000676 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.000789 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 97.2%, Avg loss: 0.001515 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.001641 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.001346 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.001296 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 0.001542 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.000985 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.000930 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.4%, Avg loss: 0.000893 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.5%, Avg loss: 0.000872 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.5%, Avg loss: 0.000855 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.000854 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.000853 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.000852 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.000850 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.000850 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mini-batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset0 = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=True, transform=transform)\n",
        "testset = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=False, transform=transform)\n",
        "\n",
        "# Also create a validation set \n",
        "trainset, valset = train_test_split(trainset0, test_size=10000, random_state=42)\n",
        "#pytorch alternative for spliting into train and validation sets\n",
        "#https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.001697 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.001694 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.001691 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.001689 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.001686 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# batch_size = 8\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.003367 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.003361 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.003358 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.003352 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.003348 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# batch_size = 4\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.006675 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.006665 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.006661 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.006654 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.006651 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# batch_size = 2\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now we choose final model, run the test set!\n",
        "Choice: epochs = 5, batch_size = 16, learning_rate = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.000703 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.000691 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.000732 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.000774 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.000786 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(testloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q9\n",
        "\n",
        "Question 9: Add some data augmentations to the data loader for the training set. Why do we only augment the training data? Play around with the augmentations available in torchvision. Try to get better performance than the baseline. Once you are happy with your choice of augmentations, run both the baseline and the augmented version on the test set and report the accuracies in your report.\n",
        "\n",
        "Notes:\n",
        "- When we use data augmentation, loading the whole data into memory creates a problem. If we use the dataloader, then the data augmentation is randomly applied each time. If we preload the data the dataloader only applies sthe data augmentation once, defeating the purpose. Here, we'll need to use pytorch's random split function instead:\n",
        "https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split\n",
        "\n",
        "- Apply this to the dataset object before passing it to the dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "pic should be Tensor or ndarray. Got <class 'PIL.Image.Image'>.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/3s/9v17gzrj1kvctdbg2vd9d2qw0000gn/T/ipykernel_89071/620508728.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Also create a validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#pytorch alternative for spliting into train and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2443\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2445\u001b[0;31m     return list(\n\u001b[0m\u001b[1;32m   2446\u001b[0m         chain.from_iterable(\n\u001b[1;32m   2447\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2445\u001b[0m     return list(\n\u001b[1;32m   2446\u001b[0m         chain.from_iterable(\n\u001b[0;32m-> 2447\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2448\u001b[0m         )\n\u001b[1;32m   2449\u001b[0m     )\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_list_indexing\u001b[0;34m(X, key, key_dtype)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;31m# key is a integer array-like of key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;31m# key is a integer array-like of key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pic should be Tensor or ndarray. Got {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: pic should be Tensor or ndarray. Got <class 'PIL.Image.Image'>."
          ]
        }
      ],
      "source": [
        "# Define a transform to normalize the data (previosly used and now only for val and test)\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# with data augmentation for training only\n",
        "train_transform = transforms.Compose(\n",
        "                    [transforms.ToPILImage(),\n",
        "                    # transforms.RandomRotation(30),\n",
        "                    transforms.RandomAffine(degrees=20, translate=(0.1,0.1), scale=(0.9, 1.1)),\n",
        "                    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset0 = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=True, transform=train_transform)\n",
        "testset = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=False, transform=transform)\n",
        "\n",
        "# Also create a validation set \n",
        "trainset, valset = train_test_split(trainset0, test_size=10000, random_state=42)\n",
        "#pytorch alternative for spliting into train and validation sets\n",
        "#https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "handwritten_digit_recognition_CPU.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
