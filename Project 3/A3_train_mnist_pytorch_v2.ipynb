{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TOyGrPT5ASDc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim, nn, unsqueeze\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "colab_type": "code",
        "id": "sZD2NGz2Ak6w",
        "outputId": "74eec0da-d867-406b-be2c-4013a2162bf1"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(17)\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset0 = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=True, transform=transform)\n",
        "testset = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=False, transform=transform)\n",
        "\n",
        "# Also create a validation set \n",
        "trainset, valset = train_test_split(trainset0, test_size=10000, random_state=42)\n",
        "#pytorch alternative for spliting into train and validation sets\n",
        "#https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "colab_type": "code",
        "id": "XCsoAdjdLjPb",
        "outputId": "6e7a5f80-f945-4e5c-c538-8ef445b6ad3e"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.conv_neural_network_layers = nn.Sequential(\n",
        "                # output_sizeOfEachConvLayer = [(in_channel + 2*padding - kernel_size) / stride] + 1\n",
        "\n",
        "                # We have in_channels=1 because our input is a grayscale image\n",
        "                nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1, stride=1), \n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size=2), \n",
        "\n",
        "                # output of second conv layer\n",
        "                nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size=2), \n",
        "\n",
        "                # output of third conv layer\n",
        "                nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size=2) )\n",
        "\n",
        "        # Linear layer\n",
        "        self.linear_layers = nn.Sequential(\n",
        "                nn.Linear(64*3*3, 10))\n",
        "                # nn.Linear(16, 10)) # The output is 10 which should match the size of our class\n",
        "\n",
        "\n",
        "    # Defining the forward pass \n",
        "    def forward(self, x):\n",
        "        x = self.conv_neural_network_layers(x)\n",
        "        # After we get the output of our convolutional layer we must flatten it or rearrange the output into a vector\n",
        "        x = torch.flatten(x, 1) # same as x = x.view(x.size(0), -1)\n",
        "        # Then pass it through the linear layer\n",
        "        x = self.linear_layers(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = Network()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss() #CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "#In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters.\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        y_hot = F.one_hot(y, 10)\n",
        "        y_hot = torch.zeros(batch_size, 10)\n",
        "        y_hot[range(y_hot.shape[0]), y]=1      \n",
        "\n",
        "        X, y_hot = X.to(device), y_hot.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y_hot)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "            \n",
        "\n",
        "#We also check the model’s performance against the test dataset to ensure it is learning.\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        y_hot = F.one_hot(y, 10)\n",
        "        y_hot = torch.zeros(batch_size, 10)\n",
        "        y_hot[range(y_hot.shape[0]), y]=1      \n",
        "\n",
        "        X, y_hot = X.to(device), y_hot.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        test_loss += loss_fn(pred, y_hot).item()\n",
        "        correct += (pred.argmax(axis=1) == y_hot.argmax(axis=1)).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= 10000\n",
        "    correct /= size\n",
        "\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fdb3a972310>\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/brunaguedes/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/Users/brunaguedes/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1295, in _shutdown_workers\n",
            "    if self._persistent_workers or self._workers_status[worker_id]:\n",
            "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            " Accuracy: 98.4%, Avg loss: 0.000798 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.000526 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.000572 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.000552 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.000513 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.000533 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.3%, Avg loss: 0.000476 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.000584 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.2%, Avg loss: 0.000547 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.2%, Avg loss: 0.000596 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.2%, Avg loss: 0.000586 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.000774 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.000723 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.000676 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.1%, Avg loss: 0.000789 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 97.2%, Avg loss: 0.001515 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.001641 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.001346 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.001296 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 0.001542 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.000985 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.000930 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.4%, Avg loss: 0.000893 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.5%, Avg loss: 0.000872 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.5%, Avg loss: 0.000855 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.000854 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.000853 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.000852 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.000850 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.000850 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mini-batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset0 = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=True, transform=transform)\n",
        "testset = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=False, transform=transform)\n",
        "\n",
        "# Also create a validation set \n",
        "trainset, valset = train_test_split(trainset0, test_size=10000, random_state=42)\n",
        "#pytorch alternative for spliting into train and validation sets\n",
        "#https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.001697 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.001694 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.001691 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.001689 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.001686 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# batch_size = 8\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.003367 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.003361 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.003358 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.003352 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.003348 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# batch_size = 4\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.006675 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.006665 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.006661 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.006654 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.006651 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# batch_size = 2\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(valloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now we choose final model, run the test set!\n",
        "Choice: epochs = 5, batch_size = 16, learning_rate = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.000703 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.000691 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.000732 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.000774 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.000786 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, model, loss_fn, optimizer)\n",
        "    test(testloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q9\n",
        "\n",
        "Question 9: Add some data augmentations to the data loader for the training set. Why do we only augment the training data? Play around with the augmentations available in torchvision. Try to get better performance than the baseline. Once you are happy with your choice of augmentations, run both the baseline and the augmented version on the test set and report the accuracies in your report.\n",
        "\n",
        "Notes:\n",
        "- When we use data augmentation, loading the whole data into memory creates a problem. If we use the dataloader, then the data augmentation is randomly applied each time. If we preload the data the dataloader only applies sthe data augmentation once, defeating the purpose. Here, we'll need to use pytorch's random split function instead:\n",
        "https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split\n",
        "\n",
        "- Apply this to the dataset object before passing it to the dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a transform to normalize the data (previosly used and now only for val and test)\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# with data augmentation for training only\n",
        "train_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    # transforms.ToPILImage(),\n",
        "                                    # transforms.CenterCrop(21),\n",
        "                                    transforms.RandomRotation(30),\n",
        "                                    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "                                    # transforms.RandomAffine(degrees=20, translate=(0.1,0.1), scale=(0.9, 1.1)),\n",
        "                                    transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "\n",
        "# Download and load the training data\n",
        "trainset0 = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=True, transform=train_transform)\n",
        "testset = datasets.MNIST('drive/My Drive/mnist/MNIST_data/', download=True, train=False, transform=transform)\n",
        "\n",
        "# Also create a validation set \n",
        "trainset_augm, valset_augm = random_split(trainset0, [50000, 10000], generator=torch.Generator().manual_seed(42))\n",
        "# trainset, valset = train_test_split(trainset0, test_size=10000, random_state=42)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "trainloader_augm = torch.utils.data.DataLoader(trainset_augm, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "valloader_augm = torch.utils.data.DataLoader(valset_augm, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader_augm = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.000683 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.000655 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.6%, Avg loss: 0.000608 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.000604 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.000621 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader_augm, model, loss_fn, optimizer)\n",
        "    test(valloader_augm, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test set for best accuracy in validation\n",
        "\n",
        "Combination of \n",
        "- transforms.RandomRotation(30),\n",
        "- transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "- transforms.Normalize((0.5,), (0.5,)\n",
        "\n",
        "also tried but had worse results:\n",
        "- adding to the previous transforms.RandomAffine(degrees=20, translate=(0.1,0.1), scale=(0.9, 1.1))\n",
        "    - Epoch 5, Test Error: Accuracy: 97.3%, Avg loss: 0.001223 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.2%, Avg loss: 0.000371 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.3%, Avg loss: 0.000357 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.3%, Avg loss: 0.000391 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.4%, Avg loss: 0.000360 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Test Error: \n",
            " Accuracy: 99.3%, Avg loss: 0.000367 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader_augm, model, loss_fn, optimizer)\n",
        "    test(testloader_augm, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "handwritten_digit_recognition_CPU.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
